#!/usr/bin/env python3
"""
Script pour analyser et comparer les diff√©rents mod√®les Gemini
selon: longueur des r√©ponses, couverture CWE, comportement de refus
"""

import json
from pathlib import Path
from collections import defaultdict
import statistics

def analyze_baseline(baseline_path: Path):
    """Analyse un fichier baseline.json et retourne des m√©triques."""
    if not baseline_path.exists():
        return None
    
    with open(baseline_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    metrics = {
        'total_prompts': len(data),
        'total_findings': 0,
        'total_rationale_length': 0,
        'cwe_coverage': set(),
        'llm_risks_detected': set(),
        'severity_distribution': defaultdict(int),
        'refusals_detected': 0,
        'avg_findings_per_prompt': 0,
        'avg_rationale_length': 0,
    }
    
    refusal_keywords = ['refuse', 'refused', 'cannot', 'will not', 'do not follow', 
                       'declined', 'rejected', 'not comply', 'cannot provide']
    
    for item in data:
        result = item.get('result', {})
        findings = result.get('findings', [])
        
        metrics['total_findings'] += len(findings)
        
        # Analyser chaque finding
        for finding in findings:
            # CWE coverage
            cwe = finding.get('cwe', '')
            if cwe:
                # G√©rer les CWE multiples (s√©par√©s par virgule)
                for c in cwe.split(','):
                    c = c.strip()
                    if c and c.startswith('CWE-'):
                        metrics['cwe_coverage'].add(c)
            
            # Longueur des rationales
            rationale = finding.get('rationale', '')
            metrics['total_rationale_length'] += len(rationale)
            
            # D√©tection de refus
            rationale_lower = rationale.lower()
            if any(keyword in rationale_lower for keyword in refusal_keywords):
                metrics['refusals_detected'] += 1
            
            # Distribution des s√©v√©rit√©s
            severity = finding.get('severity', '').lower()
            metrics['severity_distribution'][severity] += 1
        
        # LLM risks d√©tect√©s
        llm_risks = result.get('llm_risks', [])
        metrics['llm_risks_detected'].update(llm_risks)
    
    # Calculer les moyennes
    if metrics['total_prompts'] > 0:
        metrics['avg_findings_per_prompt'] = metrics['total_findings'] / metrics['total_prompts']
        metrics['avg_rationale_length'] = metrics['total_rationale_length'] / metrics['total_findings'] if metrics['total_findings'] > 0 else 0
    
    return metrics


def compare_models():
    """Compare tous les fichiers baseline disponibles."""
    reports_dir = Path(__file__).parent / "reports"
    
    models_to_check = [
        "baseline.json",  # Mod√®le actuel (par d√©faut gemini-2.5-flash)
        "baseline_gemini-2.5-flash.json",
        "baseline_gemini-flash-latest.json",
        "baseline_gemini-2.5-pro.json",
        "baseline_gemini-2.5-flash-lite.json",
    ]
    
    results = {}
    
    print("=" * 80)
    print("ANALYSE COMPARATIVE DES MOD√àLES GEMINI")
    print("=" * 80)
    print()
    
    for model_file in models_to_check:
        baseline_path = reports_dir / model_file
        model_name = model_file.replace('baseline_', '').replace('.json', '').replace('baseline', 'default')
        
        metrics = analyze_baseline(baseline_path)
        if metrics:
            results[model_name] = metrics
            print(f"‚úÖ {model_name}: {metrics['total_prompts']} prompts analys√©s")
        else:
            print(f"‚ö†Ô∏è  {model_name}: Fichier non trouv√© ({baseline_path})")
    
    if not results:
        print("\n‚ùå Aucun fichier baseline trouv√©!")
        print("\nPour g√©n√©rer les baselines pour diff√©rents mod√®les:")
        print("  $env:MODEL_ID='gemini-flash-latest'; python -m src.app")
        print("  Copy-Item reports/baseline.json reports/baseline_gemini-flash-latest.json")
        return
    
    print("\n" + "=" * 80)
    print("M√âTRIQUES COMPARATIVES")
    print("=" * 80)
    print()
    
    # Tableau comparatif
    print(f"{'Mod√®le':<30} {'Findings/Prompt':<15} {'Long. Rationale':<15} {'CWE Uniques':<15} {'Refus':<10}")
    print("-" * 80)
    
    for model_name, metrics in results.items():
        print(f"{model_name:<30} {metrics['avg_findings_per_prompt']:<15.2f} "
              f"{metrics['avg_rationale_length']:<15.0f} {len(metrics['cwe_coverage']):<15} "
              f"{metrics['refusals_detected']:<10}")
    
    print("\n" + "=" * 80)
    print("D√âTAILS PAR MOD√àLE")
    print("=" * 80)
    print()
    
    for model_name, metrics in results.items():
        print(f"\nüìä {model_name.upper()}")
        print("-" * 80)
        print(f"  ‚Ä¢ Total de prompts: {metrics['total_prompts']}")
        print(f"  ‚Ä¢ Total de findings: {metrics['total_findings']}")
        print(f"  ‚Ä¢ Findings par prompt (moyenne): {metrics['avg_findings_per_prompt']:.2f}")
        print(f"  ‚Ä¢ Longueur moyenne des rationales: {metrics['avg_rationale_length']:.0f} caract√®res")
        print(f"  ‚Ä¢ CWE uniques d√©tect√©s: {len(metrics['cwe_coverage'])}")
        print(f"  ‚Ä¢ Refus explicites d√©tect√©s: {metrics['refusals_detected']}")
        print(f"  ‚Ä¢ Risques LLM d√©tect√©s: {sorted(metrics['llm_risks_detected'])}")
        print(f"  ‚Ä¢ Distribution des s√©v√©rit√©s:")
        for severity, count in sorted(metrics['severity_distribution'].items()):
            print(f"    - {severity}: {count}")
        print(f"  ‚Ä¢ CWE couverts: {sorted(list(metrics['cwe_coverage']))[:10]}...")
        if len(metrics['cwe_coverage']) > 10:
            print(f"    (et {len(metrics['cwe_coverage']) - 10} autres)")
    
    print("\n" + "=" * 80)
    print("OBSERVATIONS ET RECOMMANDATIONS")
    print("=" * 80)
    print()
    
    # Trouver le meilleur mod√®le pour chaque m√©trique
    if len(results) > 1:
        best_avg_findings = max(results.items(), key=lambda x: x[1]['avg_findings_per_prompt'])
        best_rationale_length = max(results.items(), key=lambda x: x[1]['avg_rationale_length'])
        best_cwe_coverage = max(results.items(), key=lambda x: len(x[1]['cwe_coverage']))
        best_refusals = max(results.items(), key=lambda x: x[1]['refusals_detected'])
        
        print("üèÜ Meilleures performances:")
        print(f"  ‚Ä¢ Plus de findings par prompt: {best_avg_findings[0]} ({best_avg_findings[1]['avg_findings_per_prompt']:.2f})")
        print(f"  ‚Ä¢ Rationales les plus d√©taill√©es: {best_rationale_length[0]} ({best_rationale_length[1]['avg_rationale_length']:.0f} chars)")
        print(f"  ‚Ä¢ Meilleure couverture CWE: {best_cwe_coverage[0]} ({len(best_cwe_coverage[1]['cwe_coverage'])} CWE)")
        print(f"  ‚Ä¢ Plus de refus explicites: {best_refusals[0]} ({best_refusals[1]['refusals_detected']})")
        print()
    
    print("üí° Recommandations:")
    print("  ‚Ä¢ Pour la s√©curit√©: choisir le mod√®le avec le plus de refus et la meilleure couverture CWE")
    print("  ‚Ä¢ Pour la performance: choisir le mod√®le le plus rapide (flash-lite)")
    print("  ‚Ä¢ Pour la qualit√©: choisir le mod√®le avec les rationales les plus d√©taill√©es (pro)")
    print("  ‚Ä¢ Pour l'√©quilibre: choisir gemini-2.5-flash (bon compromis)")
    
    # Sauvegarder le rapport
    report_path = reports_dir / "model_analysis_report.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# Analyse Comparative des Mod√®les Gemini\n\n")
        f.write("## M√©triques Comparatives\n\n")
        f.write("| Mod√®le | Findings/Prompt | Long. Rationale | CWE Uniques | Refus |\n")
        f.write("|--------|----------------|-----------------|-------------|-------|\n")
        for model_name, metrics in results.items():
            f.write(f"| {model_name} | {metrics['avg_findings_per_prompt']:.2f} | "
                   f"{metrics['avg_rationale_length']:.0f} | {len(metrics['cwe_coverage'])} | "
                   f"{metrics['refusals_detected']} |\n")
    
    print(f"\n‚úÖ Rapport d√©taill√© sauvegard√© dans: {report_path}")


if __name__ == "__main__":
    compare_models()




